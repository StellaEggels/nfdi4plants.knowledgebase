---
title: Validate, Review & Publish your ARC
sidebar:
  order: 11
lastUpdated: 2025-09-16
authors:
  - saskia-hiltemann
  - kathryn-dumschott
  - sabrina-zander
  - stella-eggels
  - ursula-eberhardt
  - dominik-brilhaus
---

import { Steps } from '@astrojs/starlight/components';
import { Card } from '@astrojs/starlight/components';

import {Content as ValidateARC} from '../start-here/validate-arc.mdx';
import {Content as PublishARC} from '../start-here/publish-arc.mdx';


<ValidateARC />


## Reviewing your ARC before publication

Before an ARC can be published, it must clearly and completely describe the data it contains. In this guide we outline things to consider before submitting your ARC for publication.

### Check the ARC repository on DataHUB

First, check the ARC's top level information:

<Card icon="pen" title="DataHUB repository">
    <Steps>

    1. Open your ARC on [PLANTDataHUB](https://git.nfdi4plants.org)
    2. Is the name of the repository descriptive?
    3. The repository should be set to public. Check this by looking at the icon behind the repository name.
       - You can change the visibility under `Settings -> General -> Visibility, project features, permissions` (This is currently the best way to ensure that the people reviewing the corresponding publication have access to the ARC.)
    4. Check the `README.md`, does it contain useful information? This is the first thing people will see when they look at your ARC on DataHUB and is a useful way to 'orientate' the viewer in the ARC. (However, be sure that any important information in the README is also contained within the actual ARC structure in order to adhere to the ISA framework.)
    5. Does your repository have a `LICENSE` file? In the panel on the right you will find a button to add a license. There are many license templates available to choose from. We recommend an open license such as `MIT` or `Creative Commons`
    6. Check the **Project Storage** on the right side. Is most of the storage space LFS storage?

    </Steps>
</Card>

### Check the ARC (e.g. in ARCitect)

Next, open the ARC itself, and run through everything one last time, or else ask a friend or colleague to have a look at your ARC to make sure it's understandable.

<Card icon="pen" title="Investigation">
    <Steps>
    1. The title and description should be clear and informative.
    2. Check the contacts. Are all collaborators included? Is all data (name, email, ORCiD, affiliation) for the contacts correct and complete?

    </Steps>
</Card>

<Card icon="pen" title="Studies and Assays">
For each study and assay in your ARC, check that:
    <Steps>
    1. The top-level metadata is present and informative, including the technology metadata.
    2. Within all annotation tables:
       - Columns use ontology terms where possible.
       - Column types (e.g. characteristic, parameter, factor) are correct.
       - Fixed ISA Column headers (e.g. Protocol REF) may not be manually renamed.
       - Input column headers match the 'type' of input in the column (this can be `Input[Source Name]`, `Input[Sample Name]`, `Input[Material]`, or `Input[Data]`).
       - Output column headers match the 'type' of output in the column (this can be `Output[Source Name]`, `Output[Sample Name]`, `Output[Material]`, or `Output[Data]`).
       - Columns have units (where applicable).
       - The values in the columns make sense (e.g. numerical values in a temperature column), no obvious copy-paste errors, etc.
       - There are no missing columns.
         - Important metadata present in a protocol document has been added to the annotation table.
         - Any community requirements such as templates or validation packages are utilized.
    3. Protocol documents in the `protocols` folder must be referenced in the corresponding annotation sheet via a `Protocol REF` column.
    4. [Assays] The data contained in the `dataset` folder is organised in an understandable way AND is referenced in the 'Output [Data]' column of the corresponding annotation sheet.
    5. [Assays] TODO: check datamaps? check if one should be added?

    </Steps>
</Card>

<Card icon="pen" title="Runs and Workflows">
    <Steps>
    1. Analysis code is included either as a CWL workflow in the workflows/runs folder, or as a virtual assay.
       See also our [guide on adding data analysis](https://nfdi4plants.github.io/nfdi4plants.knowledgebase/start-here/data-analysis)
    2. Was any external data used in the analysis? Is it clear where this came from?
    3. [CWL workflow] Ensure that it runs using `cwltool`. See [guide](https://nfdi4plants.github.io/nfdi4plants.knowledgebase/start-here/data-analysis/option2-cwl/) for more details.
    4. [Virtual Assay] Code should be in the "protocols" folder, and input/output  data in "dataset" folder. Ensure that the workflow runs.

    </Steps>
</Card>

<Card icon="pen" title="All together">
To check the overall consistency of your ARC, make sure that the full connection from sample to raw data to processed data is there.
    <Steps>
    1. For every output, can you trace its origins back through the ARC? Is the provenance of all data fully described?
       - A mermaid graph is a quick way to visualize this provenance, showing how the inputs and outputs from your studies and assays are connected. Learn more about the arcIsaProcessMermaid tool [here](https://www.nuget.org/packages/arcIsaProcessMermaid/#readme-body-tab)
       - The `arc-summary.md` lists the files that are properly linked in the ARC. This is provided as a downloadable artifact by the CI/CD job "Create ARC json".
    </Steps>
</Card>



## Publish your ARC
